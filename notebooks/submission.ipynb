{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from typing import Union\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from rdkit.Chem import rdmolops, Draw\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "Molecule = Union[str, Chem.Mol]\n",
    "\n",
    "logging.getLogger('rdkit').setLevel(logging.WARNING)\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.warning')\n",
    "\n",
    "load_dotenv()\n",
    "PROJECT_ROOT = os.getenv(\"PROJECT_ROOT\") if os.getenv(\"PROJECT_ROOT\") else \"/home/elisey/dkr/F24-DKR-Project\"\n",
    "DATA_DIR = PROJECT_ROOT + \"/data/\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from rotary_embedding_torch import RotaryEmbedding, apply_rotary_emb\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "List = nn.ModuleList\n",
    "\n",
    "# normalizations\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        fn\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, *args,**kwargs)\n",
    "\n",
    "# gated residual\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def forward(self, x, res):\n",
    "        return x + res\n",
    "\n",
    "class GatedResidual(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(dim * 3, 1, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, res):\n",
    "        gate_input = torch.cat((x, res, x - res), dim = -1)\n",
    "        gate = self.proj(gate_input)\n",
    "        return x * gate + res * (1 - gate)\n",
    "\n",
    "# attention\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        pos_emb = None,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        edge_dim = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        edge_dim = default(edge_dim, dim)\n",
    "\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.pos_emb = pos_emb\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2)\n",
    "        self.edges_to_kv = nn.Linear(edge_dim, inner_dim)\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, nodes, edges, mask = None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(nodes)\n",
    "        k, v = self.to_kv(nodes).chunk(2, dim = -1)\n",
    "\n",
    "        e_kv = self.edges_to_kv(edges)\n",
    "\n",
    "        q, k, v, e_kv = map(lambda t: rearrange(t, 'b ... (h d) -> (b h) ... d', h = h), (q, k, v, e_kv))\n",
    "\n",
    "        if exists(self.pos_emb):\n",
    "            freqs = self.pos_emb(torch.arange(nodes.shape[1], device = nodes.device))\n",
    "            freqs = rearrange(freqs, 'n d -> () n d')\n",
    "            q = apply_rotary_emb(freqs, q)\n",
    "            k = apply_rotary_emb(freqs, k)\n",
    "\n",
    "        ek, ev = e_kv, e_kv\n",
    "\n",
    "        k, v = map(lambda t: rearrange(t, 'b j d -> b () j d '), (k, v))\n",
    "        k = k + ek\n",
    "        v = v + ev\n",
    "\n",
    "        sim = einsum('b i d, b i j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b i -> b i ()') & rearrange(mask, 'b j -> b () j')\n",
    "            mask = repeat(mask, 'b i j -> (b h) i j', h = h)\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b i j, b i j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# optional feedforward\n",
    "\n",
    "def FeedForward(dim, ff_mult = 4):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim, dim * ff_mult),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(dim * ff_mult, dim)\n",
    "    )\n",
    "\n",
    "# classes\n",
    "\n",
    "class GraphTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        dim_head = 64,\n",
    "        edge_dim = None,\n",
    "        heads = 8,\n",
    "        gated_residual = True,\n",
    "        with_feedforwards = False,\n",
    "        norm_edges = False,\n",
    "        rel_pos_emb = False,\n",
    "        accept_adjacency_matrix = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = List([])\n",
    "        edge_dim = default(edge_dim, dim)\n",
    "        self.norm_edges = nn.LayerNorm(edge_dim) if norm_edges else nn.Identity()\n",
    "\n",
    "        self.adj_emb = nn.Embedding(2, edge_dim) if accept_adjacency_matrix else None\n",
    "\n",
    "        pos_emb = RotaryEmbedding(dim_head) if rel_pos_emb else None\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(List([\n",
    "                List([\n",
    "                    PreNorm(dim, Attention(dim, pos_emb = pos_emb, edge_dim = edge_dim, dim_head = dim_head, heads = heads)),\n",
    "                    GatedResidual(dim)\n",
    "                ]),\n",
    "                List([\n",
    "                    PreNorm(dim, FeedForward(dim)),\n",
    "                    GatedResidual(dim)\n",
    "                ]) if with_feedforwards else None\n",
    "            ]))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        nodes,\n",
    "        edges = None,\n",
    "        adj_mat = None,\n",
    "        mask = None\n",
    "    ):\n",
    "        batch, seq, _ = nodes.shape\n",
    "\n",
    "        if exists(edges):\n",
    "            edges = self.norm_edges(edges)\n",
    "\n",
    "        if exists(adj_mat):\n",
    "            assert adj_mat.shape == (batch, seq, seq)\n",
    "            assert exists(self.adj_emb), 'accept_adjacency_matrix must be set to True'\n",
    "            adj_mat = self.adj_emb(adj_mat.long())\n",
    "\n",
    "        all_edges = default(edges, 0) + default(adj_mat, 0)\n",
    "\n",
    "        for attn_block, ff_block in self.layers:\n",
    "            attn, attn_residual = attn_block\n",
    "            nodes = attn_residual(attn(nodes, all_edges, mask = mask), nodes)\n",
    "\n",
    "            if exists(ff_block):\n",
    "                ff, ff_residual = ff_block\n",
    "                nodes = ff_residual(ff(nodes), nodes)\n",
    "\n",
    "        return nodes, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>is_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOc1ccc2nc(S(N)(=O)=O)sc2c1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCN1C(=O)NC(c2ccccc2)C1=O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC[C@]1(O)CC[C@H]2[C@@H]3CCC4=CCCC[C@@H]4[C@H]...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCCN(CC)C(CC)C(=O)Nc1c(C)cccc1C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC(O)(P(=O)(O)O)P(=O)(O)O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>CCOc1nc2cccc(C(=O)O)c2n1Cc1ccc(-c2ccccc2-c2nnn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7827</th>\n",
       "      <td>CC(=O)[C@H]1CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7828</th>\n",
       "      <td>C[C@]12CC[C@H]3[C@@H](CCC4=CC(=O)CC[C@@]43C)[C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7829</th>\n",
       "      <td>C[C@]12CC[C@@H]3c4ccc(O)cc4CC[C@H]3[C@@H]1CC[C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7830</th>\n",
       "      <td>COc1ccc2c(c1OC)CN1CCc3cc4c(cc3C1C2)OCO4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 smiles  is_toxic\n",
       "0                          CCOc1ccc2nc(S(N)(=O)=O)sc2c1         1\n",
       "1                             CCN1C(=O)NC(c2ccccc2)C1=O         0\n",
       "2     CC[C@]1(O)CC[C@H]2[C@@H]3CCC4=CCCC[C@@H]4[C@H]...         0\n",
       "3                       CCCN(CC)C(CC)C(=O)Nc1c(C)cccc1C         0\n",
       "4                             CC(O)(P(=O)(O)O)P(=O)(O)O         0\n",
       "...                                                 ...       ...\n",
       "7826  CCOc1nc2cccc(C(=O)O)c2n1Cc1ccc(-c2ccccc2-c2nnn...         0\n",
       "7827  CC(=O)[C@H]1CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(...         1\n",
       "7828  C[C@]12CC[C@H]3[C@@H](CCC4=CC(=O)CC[C@@]43C)[C...         1\n",
       "7829  C[C@]12CC[C@@H]3c4ccc(O)cc4CC[C@H]3[C@@H]1CC[C...         1\n",
       "7830            COc1ccc2c(c1OC)CN1CCc3cc4c(cc3C1C2)OCO4         1\n",
       "\n",
       "[7831 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_toxic(row): \n",
    "    return int(any(row[1:])) \n",
    "\n",
    "data = pd.read_csv(DATA_DIR + \"tox21.csv\")\n",
    "data.replace(np.nan, 0, inplace=True)\n",
    "data['is_toxic'] = data.apply(is_toxic, axis=1)\n",
    "data = data[['smiles', 'is_toxic']]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Embedding: \n",
    "    \"\"\"\n",
    "    A class used to represent the Embedding of a graph.\n",
    "\n",
    "    node_embeddings : np.ndarray\n",
    "        An array representing the embeddings of the nodes in the graph.\n",
    "    adjacency : np.ndarray\n",
    "        An adjacency matrix representing the connections between nodes in the graph.\n",
    "    degree : np.ndarray\n",
    "        An array representing the degree of each node in the graph.\n",
    "    laplacian : np.ndarray\n",
    "        A Laplacian matrix derived from the adjacency matrix of the graph.\n",
    "    edge_index : np.ndarray\n",
    "        An array representing the indices of the edges in the graph.\n",
    "    \"\"\"\n",
    "    node_embeddings: np.ndarray\n",
    "    adjacency: np.ndarray\n",
    "    degree: np.ndarray\n",
    "    laplacian: np.ndarray\n",
    "    edge_index: np.ndarray  \n",
    "    \n",
    "    def __str__(self):\n",
    "        res = f\"Node embeddings: {self.node_embeddings.shape}\\n\"\n",
    "        res += f\"Adjacency matrix: {self.adjacency.shape}\\n\"\n",
    "        res += f\"Degree matrix: {self.degree.shape}\\n\"\n",
    "        res += f\"Laplacian matrix: {self.laplacian.shape}\\n\"\n",
    "        res += f\"Edge index: {self.edge_index.shape}\"\n",
    "        \n",
    "        return res \n",
    "\n",
    "class FeatureExtractor:\n",
    "    MORGAN_RADIUS = 2\n",
    "    MORGAN_NUM_BITS = 2048\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2,fpSize=2048)\n",
    "        \n",
    "    \n",
    "    def morgan_features_generator(self, mol: Molecule, radius: int = MORGAN_RADIUS, num_bits: int = MORGAN_NUM_BITS) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generates a Morgan fingerprint for a molecule.\n",
    "\n",
    "        :param mol: A molecule (i.e., either a SMILES or an RDKit molecule).\n",
    "        :param radius: Morgan fingerprint radius.\n",
    "        :param num_bits: Number of bits in Morgan fingerprint.\n",
    "        :return: A 2D numpy array containing the Morgan fingerprint for each atom in the molecule.\n",
    "        \"\"\"\n",
    "        mol = Chem.MolFromSmiles(mol) if isinstance(mol, str) else mol\n",
    "        features = np.zeros((mol.GetNumAtoms(), num_bits))\n",
    "\n",
    "        for atom in range(mol.GetNumAtoms()):\n",
    "            env = Chem.FindAtomEnvironmentOfRadiusN(mol, radius, atom)\n",
    "            amap = {}\n",
    "            submol = Chem.PathToSubmol(mol, env, atomMap=amap)\n",
    "            Chem.GetSSSR(submol)\n",
    "            # features_vec = AllChem.GetMorganFingerprintAsBitVect(submol, radius, nBits=num_bits)\n",
    "            features_vec = self.mfpgen.GetFingerprint(submol)\n",
    "            DataStructs.ConvertToNumpyArray(features_vec, features[atom])\n",
    "\n",
    "        # (n_atoms, embedding_size)\n",
    "        return features\n",
    "    \n",
    "    def mol_to_graph(self, molecule: Chem.Mol) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Converts a molecule to its graph representation.\n",
    "        \n",
    "        :param molecule: (Chem.Mol or str): The molecule to convert. It can be either a RDKit Mol object or a SMILES string.\n",
    "        :return np.ndarray: The adjacency matrix representing the graph of the molecule.\n",
    "        \"\"\"\n",
    "        \n",
    "        mol = Chem.MolFromSmiles(molecule) if isinstance(molecule, str) else molecule\n",
    "        graph = rdmolops.GetAdjacencyMatrix(mol)\n",
    "        \n",
    "        # (n_atoms, n_atoms)\n",
    "        return graph\n",
    "    \n",
    "    def molecule_show(selff, molecule, title=False):\n",
    "        \"\"\"\n",
    "        Displays a visual representation of a molecule from its SMILES string.\n",
    "        \n",
    "        :param molecule: (str): The SMILES string representation of the molecule.\n",
    "        :param title: (bool, optional): If True, displays the SMILES string as the title of the plot. Defaults to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        m = Chem.MolFromSmiles(molecule)\n",
    "        img = Draw.MolToImage(m)\n",
    "        \n",
    "        if title: \n",
    "            plt.title(molecule)\n",
    "    \n",
    "        plt.imshow(img)\n",
    "    \n",
    "    def __call__(self, molecule: Molecule): \n",
    "        \"\"\"\n",
    "        Makes an embeddings of a molecule. \n",
    "        \n",
    "        :param molecule: (str): The SMILES string representation of the molecule.\n",
    "        :return Embedding: The embeddings of the molecule.\n",
    "        \"\"\"\n",
    "        \n",
    "        node_embeddings = self.morgan_features_generator(molecule, self.MORGAN_RADIUS, self.MORGAN_NUM_BITS)\n",
    "        adjacency = self.mol_to_graph(molecule)\n",
    "        degree = np.diag(np.sum(adjacency, axis=1))\n",
    "        laplacian = degree - adjacency\n",
    "        edge_index = np.array(np.nonzero(adjacency))\n",
    "\n",
    "        return Embedding(node_embeddings, adjacency, degree, laplacian, edge_index)\n",
    "\n",
    "feature_extractor = FeatureExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, feature_extractor: FeatureExtractor):\n",
    "        \"\"\"\n",
    "        Initializes the MoleculeDataset.\n",
    "\n",
    "        :param data: A pandas DataFrame containing the dataset with SMILES strings.\n",
    "        :param feature_extractor: An instance of FeatureExtractor to generate features.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "\n",
    "        :param idx: The index of the sample to retrieve.\n",
    "        :return: A tuple containing the node embeddings and adjacency matrix of the molecule.\n",
    "        \"\"\"\n",
    "        smiles = self.data.iloc[idx]['smiles']\n",
    "        embedding = self.feature_extractor(smiles)\n",
    "        label = self.data.iloc[idx]['is_toxic']\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(embedding.node_embeddings, dtype=torch.float32).unsqueeze(0),\n",
    "            torch.tensor(embedding.adjacency, dtype=torch.float32).unsqueeze(0),\n",
    "            torch.tensor(label, dtype=torch.float32)\n",
    "        )\n",
    "# Example usage\n",
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42, shuffle=True)\n",
    "train_dataset = MoleculeDataset(train_data, feature_extractor)\n",
    "test_dataset = MoleculeDataset(test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classifier task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, dim, depth):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.transformer = GraphTransformer(\n",
    "            dim=dim, depth=depth, accept_adjacency_matrix=True\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 1), \n",
    "        )\n",
    "    \n",
    "    def forward(self, node_embeddings, adjacency_matrix):\n",
    "        # node_embeddings: [batch_size, max_atoms, 2048]\n",
    "        # adjacency_matrix: [batch_size, max_atoms, max_atoms]\n",
    "        transformer_output = self.transformer(node_embeddings, adj_mat=adjacency_matrix)\n",
    "        # Assuming transformer_output is a tuple where the first element is the node embeddings\n",
    "        node_embeddings_transformed = transformer_output[0]\n",
    "        # Aggregate node embeddings, e.g., take mean over nodes\n",
    "        graph_embedding = node_embeddings_transformed.mean(dim=1)\n",
    "        # graph_embedding: [batch_size, 2048]\n",
    "        return self.classifier(graph_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    max_atoms = max(node_embeddings.shape[1] for node_embeddings, _, _ in batch)\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # Initialize padded tensors\n",
    "    padded_node_embeddings = torch.zeros(batch_size, max_atoms, 2048)\n",
    "    padded_adjacency_matrices = torch.zeros(batch_size, max_atoms, max_atoms)\n",
    "    labels = torch.zeros(batch_size)\n",
    "    \n",
    "    for i, (node_embeddings, adjacency_matrix, label) in enumerate(batch):\n",
    "        n_atoms = node_embeddings.shape[1]\n",
    "        padded_node_embeddings[i, :n_atoms, :] = node_embeddings.squeeze(0)\n",
    "        padded_adjacency_matrices[i, :n_atoms, :n_atoms] = adjacency_matrix.squeeze(0)\n",
    "        labels[i] = label\n",
    "    \n",
    "    return padded_node_embeddings, padded_adjacency_matrices, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_epoch(model, dataset, optimizer, criterion, device='cpu', _tqdm=False):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=custom_collate)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    bar = tqdm(dataloader, desc='Training') if _tqdm else dataloader\n",
    "    for node_embeddings, adjacency_matrix, labels in bar:\n",
    "        optimizer.zero_grad()\n",
    "        node_embeddings = node_embeddings.to(device)\n",
    "        adjacency_matrix = adjacency_matrix.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(node_embeddings, adjacency_matrix)\n",
    "        loss = criterion(outputs[:, 0], labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    mean_epoch_loss = epoch_loss / len(dataloader)\n",
    "    return mean_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataset, device='cpu', _tqdm=False):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=custom_collate)\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bar = tqdm(dataloader, desc='Validating') if _tqdm else dataloader\n",
    "        for node_embeddings, adjacency_matrix, labels in bar:\n",
    "            node_embeddings = node_embeddings.to(device)\n",
    "            adjacency_matrix = adjacency_matrix.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(node_embeddings, adjacency_matrix)\n",
    "            \n",
    "            predictions = torch.round(torch.sigmoid(outputs))\n",
    "            correct_predictions += (predictions.cpu() == labels.cpu()).sum().item()\n",
    "            total_samples += 1\n",
    "        \n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 735/735 [00:56<00:00, 13.08it/s]\n",
      "Validating: 100%|██████████| 5873/5873 [00:20<00:00, 284.22it/s]\n",
      "Validating: 100%|██████████| 1958/1958 [00:06<00:00, 289.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: Loss: 0.6668, Train Acc: 0.6620, Test Acc: 0.6879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 735/735 [00:57<00:00, 12.86it/s]\n",
      "Validating: 100%|██████████| 5873/5873 [00:21<00:00, 273.15it/s]\n",
      "Validating: 100%|██████████| 1958/1958 [00:06<00:00, 290.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: Loss: 0.6346, Train Acc: 0.6874, Test Acc: 0.7074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = TransformerClassifier(dim=2048, depth=1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs): \n",
    "    loss = train_for_epoch(model, train_dataset, optimizer, criterion, device=device, _tqdm=True)\n",
    "    train_acc = validate(model, train_dataset, device=device, _tqdm=True)\n",
    "    test_acc  = validate(model, test_dataset, device=device, _tqdm=True)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}: Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted toxicity of the molecule with SMILES CCO is: 0\n"
     ]
    }
   ],
   "source": [
    "def predict(model, feature_extractor, smiles, device='cpu'):\n",
    "    \"\"\"\n",
    "    Predicts the toxicity of a molecule given its SMILES string.\n",
    "\n",
    "    :param model: The trained model.\n",
    "    :param feature_extractor: The feature extractor used to generate embeddings.\n",
    "    :param smiles: The SMILES string of the molecule.\n",
    "    :param device: The device to run the model on (e.g., 'cpu' or 'cuda').\n",
    "    :return: The predicted toxicity (0 or 1).\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Generate embeddings for the molecule\n",
    "    embedding = feature_extractor(smiles)\n",
    "    node_embeddings = torch.tensor(embedding.node_embeddings, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    adjacency_matrix = torch.tensor(embedding.adjacency, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(node_embeddings, adjacency_matrix)\n",
    "        prediction = torch.round(torch.sigmoid(outputs)).item()\n",
    "\n",
    "    return int(prediction)\n",
    "\n",
    "new_smiles = \"CCO\" \n",
    "prediction = predict(model, feature_extractor, new_smiles, device=device)\n",
    "print(f\"The predicted toxicity of the molecule with SMILES {new_smiles} is: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
